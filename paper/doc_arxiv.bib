@article{Kilicarslan2021,
	title = {RSigELU: A nonlinear activation function for deep neural networks},
	journal = {Expert Systems with Applications},
	volume = {174},
	pages = {114805},
	year = {2021},
	issn = {0957-4174},
	doi = {https://doi.org/10.1016/j.eswa.2021.114805},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002463},
	author = {Serhat Kiliçarslan and Mete Celik},
	keywords = {Deep learning, Nonlinear activation function, Convolutional neural networks, RSigELU},
}

@inproceedings{nair2010,
	added-at = {2018-04-29T16:46:53.000+0200},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	biburl = {https://www.bibsonomy.org/bibtex/2abbd8613f8bf170ad14c553f2e0324a8/nosebrain},
	booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
	editor = {Fürnkranz, Johannes and Joachims, Thorsten},
	ee = {http://www.icml2010.org/papers/432.pdf},
	interhash = {acefcb0a5d1a937232f02f3fe0d5ab86},
	intrahash = {abbd8613f8bf170ad14c553f2e0324a8},
	keywords = {activation function net neural relu unit},
	pages = {807-814},
	timestamp = {2019-05-04T16:42:39.000+0200},
	title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
	url={https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf},
	year = 2010
}

@inproceedings{klambauer2017,
	added-at = {2020-03-06T00:00:00.000+0100},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	biburl = {https://www.bibsonomy.org/bibtex/2e21a357e9fe9af4d500ca342badee433/dblp},
	booktitle = {NIPS},
	crossref = {conf/nips/2017},
	editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	ee = {http://papers.nips.cc/paper/6698-self-normalizing-neural-networks},
	interhash = {b8d3ded222bd6b84f40327f390cdddb4},
	intrahash = {e21a357e9fe9af4d500ca342badee433},
	keywords = {dblp},
	pages = {971-980},
	timestamp = {2020-03-07T11:55:59.000+0100},
	title = {Self-Normalizing Neural Networks.},
	url = {http://dblp.uni-trier.de/db/conf/nips/nips2017.html#KlambauerUMH17},
	year = 2017
}



@inproceedings{djork2016,
	author    = {Djork{-}Arn{\'{e}} Clevert and
	Thomas Unterthiner and
	Sepp Hochreiter},
	editor    = {Yoshua Bengio and
	Yann LeCun},
	title     = {Fast and Accurate Deep Network Learning by Exponential Linear Units
	({ELUs})},
	booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
	San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
	year      = {2016},
	url       = {http://arxiv.org/abs/1511.07289},
	timestamp = {Sat, 23 Jan 2021 01:12:05 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/ClevertUH15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{xu2015,
	added-at = {2018-08-13T00:00:00.000+0200},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2018-08-14T14:28:14.000+0200},
	title = {Empirical Evaluation of Rectified Activations in Convolutional Network.},
	booktitle = {3rd International Conference on Learning Representations, {ICLR}},
	year = 2015,
	url = {https://arxiv.org/pdf/1505.00853.pdf}
}


@inproceedings{ramachandran2018,
	title	= {Searching for Activation Functions},
	author	= {Prajit Ramachandran and Barret Zoph and Quoc Le},
	year	= {2018},
	booktitle = {6th International Conference on Learning Representations, {ICLR}},
	url={https://openreview.net/forum?id=SkBYYyZRZ},
}

@misc{
	ramachandran2018searching,
	title={Searching for Activation Functions},
	author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
	year={2018},
	url={https://openreview.net/forum?id=SkBYYyZRZ},
}

@Inbook{lecun2012,
	author="LeCun, Yann A.
	and Bottou, L{\'e}on
	and Orr, Genevieve B.
	and M{\"u}ller, Klaus-Robert",
	editor="Montavon, Gr{\'e}goire
	and Orr, Genevi{\`e}ve B.
	and M{\"u}ller, Klaus-Robert",
	title="Efficient BackProp",
	bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
	year="2012",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="9--48",
	abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.",
	isbn="978-3-642-35289-8",
	doi="10.1007/978-3-642-35289-8_3"
}

@book{goodfellow2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	year={2016},
	doi={10.1007/s10710-017-9314-z}
}

@inproceedings{hendrycks2016,
	author    = {Dan Hendrycks and
	Kevin Gimpel},
	title     = {Bridging Nonlinearities and Stochastic Regularizers with Gaussian
	Error Linear Units},
	booktitle = {4th International Conference on Learning Representations, {ICLR}},
	year      = {2016},
	url       = {https://openreview.net/forum?id=Bk0MRI5lg},
}


 @InProceedings{glorot2015, title = {Deep Sparse Rectifier Neural Networks}, author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua}, booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics}, pages = {315--323}, year = {2011}, editor = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav}, volume = {15}, series = {Proceedings of Machine Learning Research}, address = {Fort Lauderdale, FL, USA}, month = {11--13 Apr}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf}, url = { http://proceedings.mlr.press/v15/glorot11a.html } } 
 
 
  @InProceedings{glorot2010, title = {Understanding the difficulty of training deep feedforward neural networks}, author = {Glorot, Xavier and Bengio, Yoshua}, booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = {249--256}, year = {2010}, editor = {Teh, Yee Whye and Titterington, Mike}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = { http://proceedings.mlr.press/v9/glorot10a.html } } 
 
@article{lu2020,
	title={Dying {ReLU} and Initialization: Theory and Numerical Examples},
	volume={28},
	ISSN={1991-7120},
	DOI={10.4208/cicp.oa-2020-0165},
	number={5},
	journal={Communications in Computational Physics},
	publisher={Global Science Press},
	author={Lu, Lu},
	year={2020},
	month={Jun},
	pages={1671–1706}
}

 @inproceedings{pascanu13, title = {On the difficulty of training recurrent neural networks}, author = {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio}, booktitle = {Proceedings of the 30th International Conference on Machine Learning}, pages = {1310--1318}, year = {2013}, editor = {Sanjoy Dasgupta and David McAllester}, volume = {28}, number = {3}, series = {Proceedings of Machine Learning Research}, address = {Atlanta, Georgia, USA}, month = {17--19 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v28/pascanu13.pdf}, url = {http://proceedings.mlr.press/v28/pascanu13.html} } 


@TECHREPORT{krizhevsky09,
	author = {Alex Krizhevsky},
	title = {Learning multiple layers of features from tiny images},
	institution = {Computer Science, University of Toronto},
	year = {2009},
	url = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}
}

@misc{lecun2010,
	author = {LeCun, Yann and Cortes, Corinna},
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	year = 2010
}



@incollection{Paszke2019,
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {Advances in Neural Information Processing Systems 32},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {8024--8035},
	year = {2019},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{frankleC19,
	added-at = {2019-07-25T00:00:00.000+0200},
	author = {Frankle, Jonathan and Carbin, Michael},
	biburl = {https://www.bibsonomy.org/bibtex/21d19a7a954f353e4f64ec5715a5d2713/dblp},
	booktitle = {7th International Conference on Learning Representations, {ICLR}},
	crossref = {conf/iclr/2019},
	ee = {https://openreview.net/forum?id=rJl-b3RcF7},
	interhash = {ec23457113db4d1703ff480d84860535},
	intrahash = {1d19a7a954f353e4f64ec5715a5d2713},
	keywords = {dblp},
	publisher = {OpenReview.net},
	timestamp = {2019-07-26T11:39:47.000+0200},
	title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.},
	url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2019.html#FrankleC19},
	year = 2019
}

@inproceedings{Kingma14,
	author = {Kingma, Diederik P. and Ba, Jimmy},
	booktitle = {3rd International Conference of Learning Representations (ICLR)},
	title = {Adam: A Method for Stochastic Optimization.},
	month = {December},
	year = {2014},
	url = {https://arxiv.org/abs/1412.6980}
}

@inproceedings{gotmare2018,
	author = {{Gotmare}, Akhilesh and {Shirish Keskar}, Nitish and {Xiong}, Caiming and {Socher}, Richard},
	title = "{A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation}",
	booktitle = {7th International Conference on Learning Representations, {ICLR}},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = 2019,
	url={https://openreview.net/forum?id=r14EOsCqKX},
}




@misc{simonyan2015,
	title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
	author={Karen Simonyan and Andrew Zisserman},
	booktitle = {3rd International Conference on Learning Representations, {ICLR}},
	year={2015},
	url={https://arxiv.org/abs/1409.1556}

}

@inproceedings{loshchilov2017,
	title={SGDR: Stochastic Gradient Descent with Warm Restarts}, 
	author={Ilya Loshchilov and Frank Hutter},
	year={2017},
	booktitle={5th International Conference of Learning Representations {ICLR}},
	url={https://ml.informatik.uni-freiburg.de/~staeglis/deploy/papers/17-ICLR-SGDR.pdf}
}

@inproceedings{Jin2016,
	author = {Jin, Xiaojie and Xu, Chunyan and Feng, Jiashi and Wei, Yunchao and Xiong, Junjun and Yan, Shuicheng},
	title = {Deep Learning with S-Shaped Rectified Linear Activation Units},
	year = {2016},
	publisher = {AAAI Press},
	url={https://arxiv.org/abs/1512.07030},
	booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
	pages = {1737–1743},
	numpages = {7},
	location = {Phoenix, Arizona},
	series = {AAAI'16}
}


@article{Srivastava2014,
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of machine learning research},
	issue_date = {January 2014},
	volume = {15},
	number = {1},
	month = {January},
	year = {2014},
	issn = {1532-4435},
	pages = {1929--1958},
	numpages = {30},
	publisher = {JMLR.org},
	url={https://jmlr.org/papers/v15/srivastava14a.html},
	keywords = {deep learning, model combination, neural networks, regularization},
} 

@inproceedings{Ioffe2015,
	author = {Ioffe, Sergey and Szegedy, Christian},
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	booktitle = {32nd  International Conference on Machine Learning - Volume 37},
	series = {International Conference of Machine Learning},
	year = {2015},
	month = {February},
	location = {Lille, France},
	pages = {448-456},
	numpages = {9},
	publisher = {JMLR.org},
	url={http://proceedings.mlr.press/v37/ioffe15.html}
} 


@inproceedings{dugas2001,
	author = {Dugas, Charles and Bengio, Yoshua and B\'{e}lisle, Fran\c{c}ois and Nadeau, Claude and Garcia, Ren\'{e}},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Leen and T. Dietterich and V. Tresp},
	pages = {},
	publisher = {MIT Press},
	title = {Incorporating Second-Order Functional Knowledge for Better Option Pricing},
	url = {https://proceedings.neurips.cc/paper/2000/file/44968aece94f667e4095002d140b5896-Paper.pdf},
	volume = {13},
	year = {2001}
}

@article{hochreiter1998,
	author = {Hochreiter, Sepp},
	title = {The Vanishing Gradient Problem during Learning Recurrent Neural Nets and Problem Solutions},
	year = {1998},
	issue_date = {April 1998},
	publisher = {World Scientific Publishing Co., Inc.},
	address = {USA},
	volume = {6},
	number = {2},
	issn = {0218-4885},
	url = {https://doi.org/10.1142/S0218488598000094},
	doi = {10.1142/S0218488598000094},
	journal = {Int. J. Uncertain. Fuzziness Knowl.-Based Syst.},
	month = apr,
	pages = {107–116},
	numpages = {10},
	keywords = {long-term dependencies, vanishing gradient, recurrent neural nets, long short-term memory}
}



@incollection{Hochreiter2001,
	author = {Hochreiter, S. and Bengio, Y. and Frasconi, P. and Schmidhuber, J.},
	booktitle = {A Field Guide to Dynamical Recurrent Neural Networks},
	publisher = {IEEE Press},
	title = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
	year = 2001,
	url={https://ml.jku.at/publications/older/ch7.pdf}
}


@article{misra2019mish,
	title={Mish: A self regularized non-monotonic neural activation function},
	author={Misra, Diganta},
	journal={arXiv preprint arXiv:1908.08681},
	year={2019}
}

@article{zhu2020,
	title = {PFLU and FPFLU: Two novel non-monotonic activation functions in convolutional neural networks},
	journal = {Neurocomputing},
	volume = {429},
	pages = {110-117},
	year = {2021},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2020.11.068},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220318749},
	author = {Meng Zhu and Weidong Min and Qi Wang and Song Zou and Xinhao Chen},
	keywords = {Convolutional Neural Network (CNN), Activation function, ReLU, Power Function Linear Unit (PFLU), Faster PFLU (FPFLU)},

}

@article{Misra2010,
	title = {Artificial neural networks in hardware: A survey of two decades of progress},
	journal = {Neurocomputing},
	volume = {74},
	number = {1},
	pages = {239-255},
	year = {2010},
	note = {Artificial Brains},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2010.03.021},
	url = {https://www.sciencedirect.com/science/article/pii/S092523121000216X},
	author = {Janardan Misra and Indranil Saha},
	keywords = {Hardware neural network, Neurochip, Parallel neural architecture, Digital neural design, Analog neural design, Hybrid neural design, Neuromorphic system, FPGA based ANN implementation, CNN implementation, RAM based implementation, Optical neural network},

}

@ARTICLE{sanchez2020,
	author={R. {Sanchez-Iborra} and A. F. {Skarmeta}},
	journal={IEEE Circuits and Systems Magazine}, 
	title={TinyML-Enabled Frugal Smart Objects: Challenges and Opportunities}, 
	year={2020},
	volume={20},
	number={3},
	pages={4-18},
	doi={10.1109/MCAS.2020.3005467}}

@inproceedings{agostinelli2014,
	author = {{Agostinelli}, Forest and {Hoffman}, Matthew and {Sadowski}, Peter and {Baldi}, Pierre},
	title = {Learning Activation Functions to Improve Deep Neural Networks},
	booktitle = {3rd International Conference on Learning Representations, 2015},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	year = 2014,
	month = dec,
}

@inproceedings{karnewar2018,
	author={Karnewar, Animesh},
	booktitle={2018 3rd International Conference for Convergence in Technology (I2CT)}, 
	title={AANN: Absolute Artificial Neural Network}, 
	year={2018},
	volume={},
	number={},
	pages={1-6},
	doi={10.1109/I2CT.2018.8529552}
}

@article{dubey2022,
	title = {Activation functions in deep learning: A comprehensive survey and benchmark},
	journal = {Neurocomputing},
	volume = {503},
	pages = {92-108},
	year = {2022},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2022.06.111},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231222008426},
	author = {Shiv Ram Dubey and Satish Kumar Singh and Bidyut Baran Chaudhuri},
	keywords = {Activation Functions, Neural networks, Convolutional neural networks, Deep learning, Overview, Recurrent Neural Networks},
	abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: https://github.com/shivram1987/ActivationFunctions.}
}